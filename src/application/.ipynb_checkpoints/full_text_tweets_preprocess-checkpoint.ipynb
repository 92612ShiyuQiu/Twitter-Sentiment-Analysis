{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Metoo preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "posts = pd.read_csv(\"../application_dataset/Metoomovement1.csv\")\n",
    "posts2 = posts.drop(posts.columns[0], axis=1)\n",
    "posts_list = posts[posts2.columns[0]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(post):\n",
    "    return re.sub(r'\\d+', '', post)\n",
    "\n",
    "posts_list = [remove_numbers(p) for p in posts_list]\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(post):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return post.translate(translator)\n",
    "\n",
    "posts_list = [remove_punctuation(p) for p in posts_list]\n",
    "\n",
    "def remove_whitespace(post):\n",
    "    return \" \".join(post.split())\n",
    "\n",
    "posts_list = [remove_whitespace(p) for p in posts_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ShiYu\n",
      "[nltk_data]     Qiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# get stop word lists\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweetTokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\ShiYu\n",
      "[nltk_data]     Qiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    return (stemmer.lemmatize(w, pos='v') for w in tweetTokenizer.tokenize(sentence.lower()) if w not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_list = [tokenizer(s) for s in posts_list]\n",
    "posts_list = [list(s) for s in posts_list]\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(posts_list, workers=8, sg=1)\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Function to average all word vectors in a post\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(tweets, model, num_features):\n",
    "    counter = 0\n",
    "    tweetFeatureVecs = np.zeros((len(tweets),num_features),dtype=\"float32\")\n",
    "    for tweet in tweets:\n",
    "        # Printing a status message every 1000th tweets\n",
    "        #if counter%1000 == 0:\n",
    "        #    print(\"Review %d of %d\"%(counter,len(tweets)))\n",
    "            \n",
    "        tweetFeatureVecs[counter] = featureVecMethod(tweet, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return tweetFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShiYu Qiu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "C:\\Users\\ShiYu Qiu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs(posts_list, model, 100)\n",
    "trainDataVecs=np.nan_to_num(trainDataVecs)\n",
    "df = pd.DataFrame(trainDataVecs)\n",
    "df.to_csv('../new_data/Metoomovement_word2vec.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trump preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"../application_dataset/trump1.csv\")\n",
    "posts2 = posts.drop(posts.columns[0], axis=1)\n",
    "posts_list = posts[posts2.columns[0]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_list = [remove_numbers(p) for p in posts_list]\n",
    "posts_list = [remove_punctuation(p) for p in posts_list]\n",
    "posts_list = [remove_whitespace(p) for p in posts_list]\n",
    "posts_list = [tokenizer(s) for s in posts_list]\n",
    "posts_list = [list(s) for s in posts_list]\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(posts_list, workers=8, sg=1)\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShiYu Qiu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "C:\\Users\\ShiYu Qiu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs(posts_list, model, 100)\n",
    "trainDataVecs=np.nan_to_num(trainDataVecs)\n",
    "df = pd.DataFrame(trainDataVecs)\n",
    "df.to_csv('../new_data/trump_word2vec.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
